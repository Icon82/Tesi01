{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1AuGRuNZrBFss6fUAB2wXrg8NZvDu0evF","authorship_tag":"ABX9TyOxP+qgx7eTpI8FftUbOCue"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":894},"id":"EKeb64D_tX-F","executionInfo":{"status":"error","timestamp":1716493465835,"user_tz":-120,"elapsed":70937,"user":{"displayName":"icon8230@gmail.com","userId":"09745553427950617310"}},"outputId":"aa7fa86e-c7bd-4715-f13a-c52bde050474"},"outputs":[{"output_type":"stream","name":"stdout","text":["Alpha: 0.09, Beta: 0.15\n","Mean Validation Loss: 0.03231295887380838\n","Alpha: 0.09, Beta: 0.16\n","Mean Validation Loss: 0.03438165318220854\n","Alpha: 0.09, Beta: 0.17\n","Mean Validation Loss: 0.037276369705796245\n","Alpha: 0.09, Beta: 0.18\n","Mean Validation Loss: 0.04727383367717266\n","Alpha: 0.1, Beta: 0.15\n","Mean Validation Loss: 0.03768234215676784\n","Alpha: 0.1, Beta: 0.16\n","Mean Validation Loss: 0.04107636120170355\n","Alpha: 0.1, Beta: 0.17\n","Mean Validation Loss: 0.04244839297607541\n","Alpha: 0.1, Beta: 0.18\n","Mean Validation Loss: 0.035214067064225675\n","Alpha: 0.12, Beta: 0.15\n","Mean Validation Loss: 0.03599969372153282\n","Alpha: 0.12, Beta: 0.16\n","Mean Validation Loss: 0.04530119821429253\n","Alpha: 0.12, Beta: 0.17\n","Mean Validation Loss: 0.03702620696276426\n","Alpha: 0.12, Beta: 0.18\n","Mean Validation Loss: 0.04847497828304768\n","Alpha: 0.13, Beta: 0.15\n","Mean Validation Loss: 0.04733216241002083\n","Alpha: 0.13, Beta: 0.16\n","Mean Validation Loss: 0.0421412693336606\n","Alpha: 0.13, Beta: 0.17\n","Mean Validation Loss: 0.037858242820948365\n","Alpha: 0.13, Beta: 0.18\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-c514083957db>\u001b[0m in \u001b[0;36m<cell line: 136>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0mval_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-c514083957db>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(X_train, y_train, X_val, y_val, X_test, y_test, alpha, beta, num_epochs)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreg_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Lettura del dataset da CSV\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data.csv')\n","\n","# Nomi delle colonne\n","input_columns = ['Anthropogenic Forcing', 'Temperature Anomaly', 'CO2', 'CH4']  # Tutte le colonne di input\n","output_columns = ['Nervous System Diseases']\n","\n","# Separazione delle caratteristiche e delle etichette\n","X = df[input_columns].values\n","y = df[output_columns].values\n","\n","# Normalizzazione dei dati climatici\n","scaler_X = StandardScaler()\n","X = scaler_X.fit_transform(X)\n","\n","# Normalizzazione delle etichette\n","scaler_y = StandardScaler()\n","y = scaler_y.fit_transform(y)\n","\n","# Funzione per creare e addestrare il modello\n","def train_model(X_train, y_train, X_val, y_val, X_test, y_test, alpha=0.5, beta=0.5, num_epochs=200):\n","    # Definizione del modello con dropout\n","    class FeedforwardNN(nn.Module):\n","        def __init__(self):\n","            super(FeedforwardNN, self).__init__()\n","            self.hidden = nn.Linear(4, 300)  # 4 input features\n","            self.hidden2 = nn.Linear(300, 100)\n","            self.dropout = nn.Dropout(0.6)\n","            self.output = nn.Linear(100, 1)   # 1 output features\n","\n","        def forward(self, x):\n","            x = torch.relu(self.hidden(x))\n","            x = torch.relu(self.hidden2(x))\n","            x = self.dropout(x)  # Aggiunta del dropout\n","            x = self.output(x)  # Usiamo linear per output continuo\n","            return x\n","\n","    model = FeedforwardNN()\n","\n","    # Definizione della funzione di perdita e dell'ottimizzatore\n","    criterion = nn.MSELoss()\n","    learning_rate = 0.0001\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Callback per Early Stopping\n","    class EarlyStopping:\n","        def __init__(self, patience=5, min_delta=0):\n","            self.patience = patience\n","            self.min_delta = min_delta\n","            self.counter = 0\n","            self.best_loss = None\n","            self.early_stop = False\n","\n","        def __call__(self, val_loss):\n","            if self.best_loss is None:\n","                self.best_loss = val_loss\n","            elif val_loss > self.best_loss - self.min_delta:\n","                self.counter += 1\n","                if self.counter >= self.patience:\n","                    self.early_stop = True\n","            else:\n","                self.best_loss = val_loss\n","                self.counter = 0\n","\n","    # Inizializzazione di Early Stopping\n","    early_stopping = EarlyStopping(patience=10, min_delta=0.001)\n","\n","    # Addestramento del modello\n","    for epoch in range(num_epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train)\n","        loss = criterion(outputs, y_train)\n","\n","        # Aggiunta del termine di regolarizzazione bayesiana\n","        reg_loss = 0\n","        for param in model.parameters():\n","            reg_loss += alpha * torch.sum(torch.log(1 + beta * torch.square(param)))\n","        loss += reg_loss\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Valutazione del modello\n","        model.eval()\n","        with torch.no_grad():\n","            val_outputs = model(X_val)\n","            val_loss = criterion(val_outputs, y_val)\n","\n","            test_outputs = model(X_test)\n","            test_loss = criterion(test_outputs, y_test)\n","\n","        # Check per early stopping\n","        early_stopping(val_loss)\n","        if early_stopping.early_stop:\n","           # print(\"Early stopping\")\n","            break\n","\n","    # Calcolo degli errori (target - output) per training, validazione e test\n","    with torch.no_grad():\n","        train_pred = model(X_train).numpy()\n","        val_pred = model(X_val).numpy()\n","        test_pred = model(X_test).numpy()\n","\n","    train_errors = y_train.numpy() - train_pred\n","    val_errors = y_val.numpy() - val_pred\n","    test_errors = y_test.numpy() - test_pred\n","\n","    # Calcolo della varianza degli errori per ogni variabile di output\n","    train_variances = np.var(train_errors, axis=0)\n","    val_variances = np.var(val_errors, axis=0)\n","    test_variances = np.var(test_errors, axis=0)\n","\n","    # Calcolo del coefficiente di correlazione per ogni variabile di output\n","    correlations = [np.corrcoef(y_test[:, i], test_pred[:, i])[0, 1] for i in range(y_test.shape[1])]\n","\n","    return loss.item(), val_loss.item(), test_loss.item(), train_variances, val_variances, test_variances, correlations\n","\n","# Esecuzione della grid search per ottimizzare alpha e beta\n","alpha_values = [0.09, 0.1, 0.12,0.13]\n","beta_values = [0.15, 0.16, 0.17,0.18]\n","best_params = None\n","best_val_loss = float('inf')\n","grid_search_results = []\n","\n","for alpha in alpha_values:\n","    for beta in beta_values:\n","        print(f'Alpha: {alpha}, Beta: {beta}')\n","        val_losses = []\n","        for run in range(10):  # Eseguiamo 5 run per ciascuna combinazione di parametri\n","            # Suddivisione dei dati in training (60%), validazione (20%) e test (20%)\n","            X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=run)\n","            X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=run)\n","\n","            # Conversione a tensori PyTorch\n","            X_train = torch.tensor(X_train, dtype=torch.float32)\n","            X_val = torch.tensor(X_val, dtype=torch.float32)\n","            X_test = torch.tensor(X_test, dtype=torch.float32)\n","            y_train = torch.tensor(y_train, dtype=torch.float32)\n","            y_val = torch.tensor(y_val, dtype=torch.float32)\n","            y_test = torch.tensor(y_test, dtype=torch.float32)\n","\n","            _, val_loss, _, _, _, _, _ = train_model(X_train, y_train, X_val, y_val, X_test, y_test, alpha=alpha, beta=beta)\n","            val_losses.append(val_loss)\n","\n","        mean_val_loss = np.mean(val_losses)\n","        grid_search_results.append([alpha, beta, mean_val_loss])\n","        print(f'Mean Validation Loss: {mean_val_loss}')\n","\n","        if mean_val_loss < best_val_loss:\n","            best_val_loss = mean_val_loss\n","            best_params = (alpha, beta)\n","\n","# Salvataggio dei risultati della grid search in un file CSV\n","grid_search_df = pd.DataFrame(grid_search_results, columns=['Alpha', 'Beta', 'Mean Validation Loss'])\n","grid_search_df.to_csv('grid_search_results.csv', index=False)\n","\n","print(f'Best Parameters: Alpha={best_params[0]}, Beta={best_params[1]}, Validation Loss={best_val_loss}')\n","\n","# Addestramento finale del modello con i migliori parametri trovati\n","final_alpha, final_beta = best_params\n","all_results = []\n","\n","for run in range(50):\n","    # Suddivisione dei dati in training (60%), validazione (20%) e test (20%)\n","    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=run)\n","    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=run)\n","\n","    # Conversione a tensori PyTorch\n","    X_train = torch.tensor(X_train, dtype=torch.float32)\n","    X_val = torch.tensor(X_val, dtype=torch.float32)\n","    X_test = torch.tensor(X_test, dtype=torch.float32)\n","    y_train = torch.tensor(y_train, dtype=torch.float32)\n","    y_val = torch.tensor(y_val, dtype=torch.float32)\n","    y_test = torch.tensor(y_test, dtype=torch.float32)\n","\n","    train_loss, val_loss, test_loss, train_variances, val_variances, test_variances, correlations = train_model(X_train, y_train, X_val, y_val, X_test, y_test, alpha=final_alpha, beta=final_beta)\n","    for i, col in enumerate(output_columns):\n","        all_results.append([run + 1, train_loss, val_loss, test_loss, train_variances[i], val_variances[i], test_variances[i], correlations[i], col])\n","\n","# Creazione del DataFrame con i risultati\n","results_df = pd.DataFrame(all_results, columns=['Run', 'Train Loss', 'Validation Loss', 'Test Loss', 'Train Variance', 'Validation Variance', 'Test Variance', 'Correlation', 'Output Variable'])\n","\n","# Salvataggio dei risultati in un file CSV\n","results_df.to_csv('all_results.csv', index=False)\n","\n","# Calcolo dei valori medi, minimi e massimi per ogni variabile di output\n","summary_df = results_df.groupby('Output Variable').agg({\n","    'Train Loss': ['mean', 'min', 'max'],\n","    'Validation Loss': ['mean', 'min', 'max'],\n","    'Test Loss': ['mean', 'min', 'max'],\n","    'Train Variance': ['mean', 'min', 'max'],\n","    'Validation Variance': ['mean', 'min', 'max'],\n","    'Test Variance': ['mean', 'min', 'max'],\n","    'Correlation': ['mean', 'min', 'max']\n","}).reset_index()\n","\n","# Rinominare le colonne per chiarezza\n","summary_df.columns = [' '.join(col).strip() for col in summary_df.columns.values]\n","\n","# Salvataggio dei valori medi, minimi e massimi in un file CSV\n","summary_df.to_csv('summary_results.csv', index=False)\n","\n","# Stampa di conferma\n","print(\"I risultati sono stati salvati in 'all_results.csv' e 'summary_results.csv'.\")\n","print(f'Best Parameters: Alpha={best_params[0]}, Beta={best_params[1]}, Validation Loss={best_val_loss}')\n","\n","\n","\n","\n"]}]}