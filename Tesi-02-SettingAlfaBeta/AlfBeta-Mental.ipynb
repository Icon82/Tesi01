{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1qE7PQrJ8mncwKszfT6VaezhqFTPjc-D_","timestamp":1716439283490},{"file_id":"1AuGRuNZrBFss6fUAB2wXrg8NZvDu0evF","timestamp":1716410147256}],"mount_file_id":"19sPbv3xfud9QxkOxD9cEET8xDeWsy4u1","authorship_tag":"ABX9TyNWHoC9wTdsaNrJHaSNPOYz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EKeb64D_tX-F","executionInfo":{"status":"ok","timestamp":1716458236465,"user_tz":-120,"elapsed":56996,"user":{"displayName":"icon8230@gmail.com","userId":"09745553427950617310"}},"outputId":"a5e08480-3c09-456a-c75a-b7e30da1152c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Alpha: 0.58, Beta: 0.0115\n","Mean Validation Loss: 0.01943722078576684\n","Alpha: 0.58, Beta: 0.012\n","Mean Validation Loss: 0.027537067304365337\n","Alpha: 0.58, Beta: 0.0125\n","Mean Validation Loss: 0.02152633140794933\n","Alpha: 0.58, Beta: 0.013\n","Mean Validation Loss: 0.03288430878892541\n","Alpha: 0.59, Beta: 0.0115\n","Mean Validation Loss: 0.023623011773452163\n","Alpha: 0.59, Beta: 0.012\n","Mean Validation Loss: 0.02881516139023006\n","Alpha: 0.59, Beta: 0.0125\n","Mean Validation Loss: 0.014768365118652583\n","Alpha: 0.59, Beta: 0.013\n","Mean Validation Loss: 0.02669909920077771\n","Alpha: 0.6, Beta: 0.0115\n","Mean Validation Loss: 0.02246388723142445\n","Alpha: 0.6, Beta: 0.012\n","Mean Validation Loss: 0.03722809855826199\n","Alpha: 0.6, Beta: 0.0125\n","Mean Validation Loss: 0.032446001563221215\n","Alpha: 0.6, Beta: 0.013\n","Mean Validation Loss: 0.024037303775548934\n","Alpha: 0.61, Beta: 0.0115\n","Mean Validation Loss: 0.026737666688859463\n","Alpha: 0.61, Beta: 0.012\n","Mean Validation Loss: 0.018570533767342568\n","Alpha: 0.61, Beta: 0.0125\n","Mean Validation Loss: 0.017180147161707283\n","Alpha: 0.61, Beta: 0.013\n","Mean Validation Loss: 0.016334187472239137\n","Alpha: 0.62, Beta: 0.0115\n","Mean Validation Loss: 0.027620698697865008\n","Alpha: 0.62, Beta: 0.012\n","Mean Validation Loss: 0.018878751201555133\n","Alpha: 0.62, Beta: 0.0125\n","Mean Validation Loss: 0.018962714285589755\n","Alpha: 0.62, Beta: 0.013\n","Mean Validation Loss: 0.020885649346746506\n","Best Parameters: Alpha=0.59, Beta=0.0125, Validation Loss=0.014768365118652583\n","I risultati sono stati salvati in 'all_results.csv' e 'summary_results.csv'.\n","Best Parameters: Alpha=0.59, Beta=0.0125, Validation Loss=0.014768365118652583\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Lettura del dataset da CSV\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data.csv')\n","\n","# Nomi delle colonne\n","input_columns = ['Anthropogenic Forcing', 'CO2', 'CH4']  # Tutte le colonne di input\n","output_columns = ['Mental and Behavioural Disorder']\n","\n","# Separazione delle caratteristiche e delle etichette\n","X = df[input_columns].values\n","y = df[output_columns].values\n","\n","# Normalizzazione dei dati climatici\n","scaler_X = StandardScaler()\n","X = scaler_X.fit_transform(X)\n","\n","# Normalizzazione delle etichette\n","scaler_y = StandardScaler()\n","y = scaler_y.fit_transform(y)\n","\n","# Funzione per creare e addestrare il modello\n","def train_model(X_train, y_train, X_val, y_val, X_test, y_test, alpha=0.5, beta=0.5, num_epochs=200):\n","    # Definizione del modello con dropout\n","    class FeedforwardNN(nn.Module):\n","        def __init__(self):\n","            super(FeedforwardNN, self).__init__()\n","            self.hidden = nn.Linear(3, 250)  # 2 input features\n","            self.dropout = nn.Dropout(0.2)\n","            self.output = nn.Linear(250, 1)  # 5 output features\n","\n","        def forward(self, x):\n","            x = torch.relu(self.hidden(x))\n","            x = self.dropout(x)  # Aggiunta del dropout\n","            x = self.output(x)  # Usiamo linear per output continuo\n","            return x\n","\n","    model = FeedforwardNN()\n","\n","    # Definizione della funzione di perdita e dell'ottimizzatore\n","    criterion = nn.MSELoss()\n","    learning_rate = 0.0004\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Callback per Early Stopping\n","    class EarlyStopping:\n","        def __init__(self, patience=5, min_delta=0):\n","            self.patience = patience\n","            self.min_delta = min_delta\n","            self.counter = 0\n","            self.best_loss = None\n","            self.early_stop = False\n","\n","        def __call__(self, val_loss):\n","            if self.best_loss is None:\n","                self.best_loss = val_loss\n","            elif val_loss > self.best_loss - self.min_delta:\n","                self.counter += 1\n","                if self.counter >= self.patience:\n","                    self.early_stop = True\n","            else:\n","                self.best_loss = val_loss\n","                self.counter = 0\n","\n","    # Inizializzazione di Early Stopping\n","    early_stopping = EarlyStopping(patience=10, min_delta=0.001)\n","\n","    # Addestramento del modello\n","    for epoch in range(num_epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(X_train)\n","        loss = criterion(outputs, y_train)\n","\n","        # Aggiunta del termine di regolarizzazione bayesiana\n","        reg_loss = 0\n","        for param in model.parameters():\n","            reg_loss += alpha * torch.sum(torch.log(1 + beta * torch.square(param)))\n","        loss += reg_loss\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Valutazione del modello\n","        model.eval()\n","        with torch.no_grad():\n","            val_outputs = model(X_val)\n","            val_loss = criterion(val_outputs, y_val)\n","\n","            test_outputs = model(X_test)\n","            test_loss = criterion(test_outputs, y_test)\n","\n","        # Check per early stopping\n","        early_stopping(val_loss)\n","        if early_stopping.early_stop:\n","           # print(\"Early stopping\")\n","            break\n","\n","    # Calcolo degli errori (target - output) per training, validazione e test\n","    with torch.no_grad():\n","        train_pred = model(X_train).numpy()\n","        val_pred = model(X_val).numpy()\n","        test_pred = model(X_test).numpy()\n","\n","    train_errors = y_train.numpy() - train_pred\n","    val_errors = y_val.numpy() - val_pred\n","    test_errors = y_test.numpy() - test_pred\n","\n","    # Calcolo della varianza degli errori per ogni variabile di output\n","    train_variances = np.var(train_errors, axis=0)\n","    val_variances = np.var(val_errors, axis=0)\n","    test_variances = np.var(test_errors, axis=0)\n","\n","    # Calcolo del coefficiente di correlazione per ogni variabile di output\n","    correlations = [np.corrcoef(y_test[:, i], test_pred[:, i])[0, 1] for i in range(y_test.shape[1])]\n","\n","    return loss.item(), val_loss.item(), test_loss.item(), train_variances, val_variances, test_variances, correlations\n","\n","# Esecuzione della grid search per ottimizzare alpha e beta\n","alpha_values = [0.58, 0.59,0.60,0.61,0.62]\n","beta_values =  [0.0115,0.012,0.0125,0.013]\n","best_params = None\n","best_val_loss = float('inf')\n","grid_search_results = []\n","\n","for alpha in alpha_values:\n","    for beta in beta_values:\n","        print(f'Alpha: {alpha}, Beta: {beta}')\n","        val_losses = []\n","        for run in range(10):  # Eseguiamo 5 run per ciascuna combinazione di parametri\n","            # Suddivisione dei dati in training (60%), validazione (20%) e test (20%)\n","            X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=run)\n","            X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=run)\n","\n","            # Conversione a tensori PyTorch\n","            X_train = torch.tensor(X_train, dtype=torch.float32)\n","            X_val = torch.tensor(X_val, dtype=torch.float32)\n","            X_test = torch.tensor(X_test, dtype=torch.float32)\n","            y_train = torch.tensor(y_train, dtype=torch.float32)\n","            y_val = torch.tensor(y_val, dtype=torch.float32)\n","            y_test = torch.tensor(y_test, dtype=torch.float32)\n","\n","            _, val_loss, _, _, _, _, _ = train_model(X_train, y_train, X_val, y_val, X_test, y_test, alpha=alpha, beta=beta)\n","            val_losses.append(val_loss)\n","\n","        mean_val_loss = np.mean(val_losses)\n","        grid_search_results.append([alpha, beta, mean_val_loss])\n","        print(f'Mean Validation Loss: {mean_val_loss}')\n","\n","        if mean_val_loss < best_val_loss:\n","            best_val_loss = mean_val_loss\n","            best_params = (alpha, beta)\n","\n","# Salvataggio dei risultati della grid search in un file CSV\n","grid_search_df = pd.DataFrame(grid_search_results, columns=['Alpha', 'Beta', 'Mean Validation Loss'])\n","grid_search_df.to_csv('grid_search_results.csv', index=False)\n","\n","print(f'Best Parameters: Alpha={best_params[0]}, Beta={best_params[1]}, Validation Loss={best_val_loss}')\n","\n","# Addestramento finale del modello con i migliori parametri trovati\n","final_alpha, final_beta = best_params\n","all_results = []\n","\n","for run in range(50):\n","    # Suddivisione dei dati in training (60%), validazione (20%) e test (20%)\n","    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=run)\n","    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=run)\n","\n","    # Conversione a tensori PyTorch\n","    X_train = torch.tensor(X_train, dtype=torch.float32)\n","    X_val = torch.tensor(X_val, dtype=torch.float32)\n","    X_test = torch.tensor(X_test, dtype=torch.float32)\n","    y_train = torch.tensor(y_train, dtype=torch.float32)\n","    y_val = torch.tensor(y_val, dtype=torch.float32)\n","    y_test = torch.tensor(y_test, dtype=torch.float32)\n","\n","    train_loss, val_loss, test_loss, train_variances, val_variances, test_variances, correlations = train_model(X_train, y_train, X_val, y_val, X_test, y_test, alpha=final_alpha, beta=final_beta)\n","    for i, col in enumerate(output_columns):\n","        all_results.append([run + 1, train_loss, val_loss, test_loss, train_variances[i], val_variances[i], test_variances[i], correlations[i], col])\n","\n","# Creazione del DataFrame con i risultati\n","results_df = pd.DataFrame(all_results, columns=['Run', 'Train Loss', 'Validation Loss', 'Test Loss', 'Train Variance', 'Validation Variance', 'Test Variance', 'Correlation', 'Output Variable'])\n","\n","# Salvataggio dei risultati in un file CSV\n","results_df.to_csv('all_results.csv', index=False)\n","\n","# Calcolo dei valori medi, minimi e massimi per ogni variabile di output\n","summary_df = results_df.groupby('Output Variable').agg({\n","    'Train Loss': ['mean', 'min', 'max'],\n","    'Validation Loss': ['mean', 'min', 'max'],\n","    'Test Loss': ['mean', 'min', 'max'],\n","    'Train Variance': ['mean', 'min', 'max'],\n","    'Validation Variance': ['mean', 'min', 'max'],\n","    'Test Variance': ['mean', 'min', 'max'],\n","    'Correlation': ['mean', 'min', 'max']\n","}).reset_index()\n","\n","# Rinominare le colonne per chiarezza\n","summary_df.columns = [' '.join(col).strip() for col in summary_df.columns.values]\n","\n","# Salvataggio dei valori medi, minimi e massimi in un file CSV\n","summary_df.to_csv('summary_results.csv', index=False)\n","\n","# Stampa di conferma\n","print(\"I risultati sono stati salvati in 'all_results.csv' e 'summary_results.csv'.\")\n","print(f'Best Parameters: Alpha={best_params[0]}, Beta={best_params[1]}, Validation Loss={best_val_loss}')\n","\n","\n","\n","\n"]}]}